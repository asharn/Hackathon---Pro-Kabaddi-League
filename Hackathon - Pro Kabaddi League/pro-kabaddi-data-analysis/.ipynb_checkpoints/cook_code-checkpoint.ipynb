{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' This '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 300)\n",
    "pd.set_option(\"display.max_rows\", 300)\n",
    "\n",
    "# Until fuction: line seperator\n",
    "def print_dashes_and_ln():\n",
    "    print('-'*100, '\\n')\n",
    "    \n",
    "# Formatter to display all float format in 2 decimal format\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "column_names = df.columns\n",
    "print(column_names)\n",
    "\n",
    "# Also check if the column is unique\n",
    "for i in column_names:\n",
    "  print('{} is unique: {}'.format(i, df[i].is_unique))\n",
    "\n",
    "'''The inplace=True has been added so you don’t need to save over the original df by assigning the result of .drop() to df. \n",
    "Many of the methods in pandas support inplace=True, so try to use it as much as possible to avoid unnecessary reassignment.'''\n",
    "# Drop unwanted columns\n",
    "df.drop(columns_to_drop, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "# Fill NaN with the mean of the column\n",
    "df['col'] = df['col'].fillna(df['col'].mean())\n",
    "\n",
    "# Drop any rows which have any nans\n",
    "df.dropna()\n",
    "# Drop columns that have any nans\n",
    "df.dropna(axis=1)\n",
    "# Only drop columns which have at least 90% non-NaNs\n",
    "df.dropna(thresh=int(df.shape[0] * .9), axis=1)\n",
    "\n",
    "# Follow this syntax\n",
    "np.where(if_this_condition_is_true, do_this, else_this)\n",
    "# Example\n",
    "df['new_column'] = np.where(df[i] > 10, 'foo', 'bar) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import dedupe\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "from unidecode import unidecode\n",
    "def preProcess(column):\n",
    "    '''\n",
    "    Used to prevent errors during the dedupe process.\n",
    "    '''\n",
    "    try : \n",
    "        column = column.decode('utf8')\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    column = unidecode(column)\n",
    "    column = re.sub('  +', ' ', column)\n",
    "    column = re.sub('\\n', ' ', column)\n",
    "    column = column.strip().strip('\"').strip(\"'\").lower().strip()\n",
    "    \n",
    "    if not column:\n",
    "        column = None\n",
    "    return column\n",
    "\n",
    "\n",
    "def readData(filename):\n",
    "    \n",
    "    data_d = {}\n",
    "    with open(filename) as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            clean_row = [(k, preProcess(v)) for (k, v) in row.items()]\n",
    "            row_id = int(row['Id'])\n",
    "            data_d[row_id] = dict(clean_row)\n",
    "return df\n",
    "name_of_file = 'data.csv'\n",
    "print('Cleaning and importing data ... ')\n",
    "df = readData(name_of_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated(['ID'], keep=False)]\n",
    "\n",
    "it'll return all duplicated rows back to you.\n",
    "\n",
    "keep : {‘first’, ‘last’, False}, default ‘first’\n",
    "\n",
    "first : Mark duplicates as True except for the first occurrence.\n",
    "last : Mark duplicates as True except for the last occurrence.\n",
    "False : Mark all duplicates as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extr = df['Date of Publication'].str.extract(r'^(\\d{4})', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date of Publication'].isnull().sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Place of Publication'] = np.where(london, 'London',\n",
    "                                      np.where(oxford, 'Oxford',\n",
    "                                               pub.str.replace('-', ' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Group of column to clean data or drop by using standard drop functionality\n",
    "preview[19:38]\n",
    "loans_2007 = loans_2007.drop('pymnt_plan', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove Columns with only One Value\n",
    "loans_2007 = loans_2007.loc[:,loans_2007.apply(pd.Series.nunique) != 1]\n",
    "\n",
    "\n",
    "Multiple check\n",
    "for col in loans_2007.columns:\n",
    "    if (len(loans_2007[col].unique()) < 4):\n",
    "    print(loans_2007[col].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "First, use the Pandas DataFrame method isnull() to return a DataFrame containing Boolean values:\n",
    "True if the original value is null\n",
    "False if the original value isn’t null\n",
    "Then, use the Pandas DataFrame method sum() to calculate the number of null values in each column.\n",
    "\n",
    "\n",
    "\n",
    "null_counts = filtered_loans.isnull().sum()\n",
    "print(\"Number of null values in each column:\\n{}\".format(null_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Investigate Categorical Columns\n",
    "print(\"Data types and their frequency\\n{}\".format(filtered_loans.dtypes.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Use the str.rstrip() string method to strip the right trailing percent sign (%).\n",
    "On the resulting Series object, use the astype() method to convert to the type float.\n",
    "Assign the new Series of float values back to the revol_util column in the filtered_loans.\n",
    "\n",
    "filtered_loans['revol_util'] = filtered_loans['revol_util'].str.rstrip('%').astype('float')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Convert Categorical Columns to Numeric Features\n",
    "\n",
    "\n",
    "mapping_dict = {\n",
    "\"emp_length\": {\n",
    "\"10+ years\": 10,\n",
    "\"9 years\": 9,\n",
    "\"8 years\": 8,\n",
    "\"7 years\": 7,\n",
    "\"6 years\": 6,\n",
    "\"5 years\": 5,\n",
    "\"4 years\": 4,\n",
    "\"3 years\": 3,\n",
    "\"2 years\": 2,\n",
    "\"1 year\": 1,\n",
    "\"< 1 year\": 0,\n",
    "\"n/a\": 0\n",
    "},\n",
    "\"grade\":{\n",
    "\"A\": 1,\n",
    "\"B\": 2,\n",
    "\"C\": 3,\n",
    "\"D\": 4,\n",
    "\"E\": 5,\n",
    "\"F\": 6,\n",
    "\"G\": 7\n",
    "}\n",
    "}\n",
    "filtered_loans = filtered_loans.replace(mapping_dict)\n",
    "filtered_loans[['emp_length','grade']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"HPI\":[80,90,70,60],\"Int_Rate\":[2,1,2,3], \"IND_GDP\":[50,45,45,67]}, index=[2001, 2002,2003,2004])\n",
    " \n",
    "df2 = pd.DataFrame({\"HPI\":[80,90,70,60],\"Int_Rate\":[2,1,2,3],\"IND_GDP\":[50,45,45,67]}, index=[2005, 2006,2007,2008])\n",
    " \n",
    "merged= pd.merge(df1,df2,on =\"HPI\")\n",
    " \n",
    "print(merged)\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"Int_Rate\":[2,1,2,3], \"IND_GDP\":[50,45,45,67]}, index=[2001, 2002,2003,2004])\n",
    " \n",
    "df2 = pd.DataFrame({\"Low_Tier_HPI\":[50,45,67,34],\"Unemployment\":[1,3,5,6]}, index=[2001, 2003,2004,2004])\n",
    " \n",
    "joined= df1.join(df2)\n",
    "print(joined)\n",
    "\n",
    "\n",
    "df1 = pd.DataFrame({\"HPI\":[80,90,70,60],\"Int_Rate\":[2,1,2,3], \"IND_GDP\":[50,45,45,67]}, index=[2001, 2002,2003,2004])\n",
    " \n",
    "df2 = pd.DataFrame({\"HPI\":[80,90,70,60],\"Int_Rate\":[2,1,2,3],\"IND_GDP\":[50,45,45,67]}, index=[2005, 2006,2007,2008])\n",
    " \n",
    "concat= pd.concat([df1,df2])\n",
    " \n",
    "print(concat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
